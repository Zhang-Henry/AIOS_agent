/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Attack started at: 2024-07-21 04:46
Namespace(llm_name='apple/DCLM-7B', max_gpu_memory={'0': '24GB', '1': '24GB', '2': '24GB', '3': '24GB'}, eval_device='cuda:0', max_new_tokens=256, scheduler_log_mode='console', agent_log_mode='console', llm_kernel_log_mode='console', use_backend=None, workflow_mode='automatic', attacker_tools_path='data/test.jsonl', tasks_path='data/agent_tasks/example/academic_agent_task.txt', agent_path='example/academic_agent_attack', observation_prompt_injection=True, plan_attack=False, action_attack=False, direct_prompt_injection=False, attack_type='naive')
Traceback (most recent call last):
  File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 982, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 684, in __getitem__
    raise KeyError(key)
KeyError: 'openlm'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/common/users/hz624/AgentAttackBench/main_attacker.py", line 156, in <module>
    main()
  File "/common/users/hz624/AgentAttackBench/main_attacker.py", line 74, in main
    llm = llms.LLMKernel(
          ^^^^^^^^^^^^^^^
  File "/common/users/hz624/AgentAttackBench/aios/llm_core/llms.py", line 48, in __init__
    self.model = OpenLLM(
                 ^^^^^^^^
  File "/common/users/hz624/AgentAttackBench/aios/llm_core/llm_classes/base_llm.py", line 33, in __init__
    self.load_llm_and_tokenizer()
  File "/common/users/hz624/AgentAttackBench/aios/llm_core/llm_classes/huggingface_native_llm.py", line 24, in load_llm_and_tokenizer
    self.model = MODEL_CLASS[self.model_type].from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 524, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 984, in from_pretrained
    raise ValueError(
ValueError: The checkpoint you are trying to load has model type `openlm` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
