/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Downloading shards:   0%|          | 0/59 [00:00<?, ?it/s]Downloading shards:   7%|▋         | 4/59 [01:07<15:27, 16.87s/it]Downloading shards:   8%|▊         | 5/59 [01:33<17:18, 19.23s/it]Downloading shards:  10%|█         | 6/59 [02:18<23:11, 26.25s/it]Downloading shards:  12%|█▏        | 7/59 [02:51<24:22, 28.13s/it]Downloading shards:  14%|█▎        | 8/59 [03:15<23:02, 27.12s/it]Downloading shards:  15%|█▌        | 9/59 [03:36<21:00, 25.22s/it]Downloading shards:  17%|█▋        | 10/59 [04:46<31:20, 38.38s/it]Downloading shards:  19%|█▊        | 11/59 [08:04<1:08:23, 85.48s/it]Downloading shards:  20%|██        | 12/59 [09:17<1:04:09, 81.91s/it]Downloading shards:  22%|██▏       | 13/59 [09:55<52:40, 68.70s/it]  Downloading shards:  24%|██▎       | 14/59 [10:20<41:48, 55.75s/it]Downloading shards:  25%|██▌       | 15/59 [10:59<37:15, 50.81s/it]Downloading shards:  27%|██▋       | 16/59 [11:35<33:08, 46.25s/it]Downloading shards:  29%|██▉       | 17/59 [12:06<29:06, 41.59s/it]Downloading shards:  31%|███       | 18/59 [12:28<24:26, 35.78s/it]Downloading shards:  32%|███▏      | 19/59 [12:55<22:03, 33.08s/it]Downloading shards:  34%|███▍      | 20/59 [17:57<1:13:57, 113.78s/it]Downloading shards:  36%|███▌      | 21/59 [19:10<1:04:25, 101.72s/it]Downloading shards:  37%|███▋      | 22/59 [20:30<58:41, 95.17s/it]   Downloading shards:  39%|███▉      | 23/59 [21:42<52:50, 88.07s/it]Downloading shards:  41%|████      | 24/59 [22:56<48:57, 83.94s/it]Downloading shards:  42%|████▏     | 25/59 [25:17<57:09, 100.87s/it]Downloading shards:  44%|████▍     | 26/59 [28:25<1:09:52, 127.04s/it]Downloading shards:  46%|████▌     | 27/59 [29:39<59:16, 111.14s/it]  Downloading shards:  47%|████▋     | 28/59 [30:49<51:07, 98.94s/it] Downloading shards:  49%|████▉     | 29/59 [31:58<44:59, 89.98s/it]Downloading shards:  51%|█████     | 30/59 [33:10<40:51, 84.53s/it]Downloading shards:  53%|█████▎    | 31/59 [33:42<32:03, 68.69s/it]Downloading shards:  54%|█████▍    | 32/59 [37:54<55:41, 123.76s/it]Downloading shards:  56%|█████▌    | 33/59 [39:10<47:23, 109.35s/it]Downloading shards:  58%|█████▊    | 34/59 [40:24<41:10, 98.82s/it] Downloading shards:  59%|█████▉    | 35/59 [41:42<37:00, 92.53s/it]Downloading shards:  61%|██████    | 36/59 [42:54<33:10, 86.54s/it]Downloading shards:  63%|██████▎   | 37/59 [44:53<35:16, 96.18s/it]Downloading shards:  64%|██████▍   | 38/59 [48:02<43:24, 124.02s/it]Downloading shards:  66%|██████▌   | 39/59 [49:13<36:00, 108.00s/it]Downloading shards:  68%|██████▊   | 40/59 [49:45<26:59, 85.24s/it] Downloading shards:  69%|██████▉   | 41/59 [50:17<20:46, 69.25s/it]Downloading shards:  71%|███████   | 42/59 [51:24<19:25, 68.56s/it]Downloading shards:  73%|███████▎  | 43/59 [52:33<18:22, 68.88s/it]Downloading shards:  75%|███████▍  | 44/59 [53:41<17:06, 68.45s/it]Downloading shards:  76%|███████▋  | 45/59 [57:53<28:49, 123.54s/it]Downloading shards:  78%|███████▊  | 46/59 [59:04<23:21, 107.80s/it]Downloading shards:  80%|███████▉  | 47/59 [1:00:13<19:13, 96.10s/it]Downloading shards:  81%|████████▏ | 48/59 [1:01:24<16:16, 88.78s/it]Downloading shards:  83%|████████▎ | 49/59 [1:02:33<13:48, 82.86s/it]Downloading shards:  85%|████████▍ | 50/59 [1:03:42<11:48, 78.70s/it]Downloading shards:  86%|████████▋ | 51/59 [1:07:54<17:24, 130.59s/it]Downloading shards:  88%|████████▊ | 52/59 [1:09:11<13:21, 114.44s/it]Downloading shards:  90%|████████▉ | 53/59 [1:10:30<10:23, 103.93s/it]Downloading shards:  92%|█████████▏| 54/59 [1:11:50<08:03, 96.75s/it] Downloading shards:  93%|█████████▎| 55/59 [1:13:09<06:05, 91.45s/it]Downloading shards:  95%|█████████▍| 56/59 [1:16:02<05:47, 115.75s/it]Downloading shards:  97%|█████████▋| 57/59 [1:18:43<04:18, 129.42s/it]Downloading shards:  98%|█████████▊| 58/59 [1:20:01<01:53, 113.85s/it]Downloading shards: 100%|██████████| 59/59 [1:20:19<00:00, 85.28s/it] Downloading shards: 100%|██████████| 59/59 [1:20:19<00:00, 81.69s/it]
Loading checkpoint shards:   0%|          | 0/59 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/59 [00:11<?, ?it/s]
Traceback (most recent call last):
  File "/common/users/hz624/AgentAttackBench/main_attacker.py", line 126, in <module>
    main()
  File "/common/users/hz624/AgentAttackBench/main_attacker.py", line 53, in main
    llm = llms.LLMKernel(
          ^^^^^^^^^^^^^^^
  File "/common/users/hz624/AgentAttackBench/aios/llm_core/llms.py", line 48, in __init__
    self.model = OpenLLM(
                 ^^^^^^^^
  File "/common/users/hz624/AgentAttackBench/aios/llm_core/llm_classes/base_llm.py", line 33, in __init__
    self.load_llm_and_tokenizer()
  File "/common/users/hz624/AgentAttackBench/aios/llm_core/llm_classes/huggingface_native_llm.py", line 24, in load_llm_and_tokenizer
    self.model = MODEL_CLASS[self.model_type].from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/transformers/modeling_utils.py", line 3838, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4298, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/transformers/modeling_utils.py", line 895, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 400, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU  has a total capacity of 44.52 GiB of which 100.81 MiB is free. Process 496667 has 40.64 GiB memory in use. Including non-PyTorch memory, this process has 3.77 GiB memory in use. Of the allocated memory 3.36 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
