The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /filer/tmp1/hz624/.cache/huggingface/hub/token
Login successful
Attack started at: 2024-08-28 10:25
Namespace(llm_name='meta-llama/Meta-Llama-3.1-70B-Instruct', max_gpu_memory={'0': '24GB', '1': '24GB', '2': '24GB', '3': '24GB', '4': '24GB', '5': '24GB', '6': '24GB', '7': '24GB'}, eval_device='["cuda:7", "cuda:6", "cuda:5", "cuda:4", "cuda:3", "cuda:2", "cuda:1", "cuda:0"]', max_new_tokens=1024, scheduler_log_mode='console', agent_log_mode='console', llm_kernel_log_mode='console', use_backend='vllm', workflow_mode='automatic', attacker_tools_path='data/all_attack_tools_non_aggressive.jsonl', tasks_path='data/agent_task.jsonl', tools_info_path='data/all_normal_tools.jsonl', observation_prompt_injection=False, plan_attack=False, action_attack=False, direct_prompt_injection=False, jailbreak=False, attack_type='naive', cot_backdoor=False, cot_clean=False, target='ResearcherProfileManager', trigger='in light of extant literature', memory_attack=False, clean=True, result_file='logs/clean/Meta-Llama-3.1-70B-Instruct/result_statistics.log', database='memory_db/chroma_db', task_num=1)
INFO 08-28 10:25:49 config.py:715] Defaulting to use mp for distributed inference
INFO 08-28 10:25:49 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='meta-llama/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/filer/tmp1/hz624/.cache/huggingface/hub/', load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 08-28 10:25:50 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=3592119)[0;0m INFO 08-28 10:25:50 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=3592117)[0;0m INFO 08-28 10:25:50 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=3592121)[0;0m INFO 08-28 10:25:50 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=3592123)[0;0m INFO 08-28 10:25:50 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=3592125)[0;0m INFO 08-28 10:25:50 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=3592127)[0;0m INFO 08-28 10:25:50 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=3592129)[0;0m INFO 08-28 10:25:50 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=3592119)[0;0m INFO 08-28 10:25:56 utils.py:784] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3592117)[0;0m INFO 08-28 10:25:56 utils.py:784] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3592129)[0;0m INFO 08-28 10:25:56 utils.py:784] Found nccl from library libnccl.so.2
INFO 08-28 10:25:56 utils.py:784] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3592123)[0;0m INFO 08-28 10:25:56 utils.py:784] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3592121)[0;0m INFO 08-28 10:25:56 utils.py:784] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3592127)[0;0m INFO 08-28 10:25:56 utils.py:784] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3592125)[0;0m INFO 08-28 10:25:56 utils.py:784] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3592119)[0;0m INFO 08-28 10:25:56 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=3592117)[0;0m INFO 08-28 10:25:56 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=3592129)[0;0m INFO 08-28 10:25:56 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 08-28 10:25:56 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=3592123)[0;0m INFO 08-28 10:25:56 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=3592121)[0;0m INFO 08-28 10:25:56 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=3592127)[0;0m INFO 08-28 10:25:56 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=3592125)[0;0m INFO 08-28 10:25:56 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=3592127)[0;0m WARNING 08-28 10:25:58 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=3592123)[0;0m WARNING 08-28 10:25:58 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=3592121)[0;0m WARNING 08-28 10:25:58 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=3592117)[0;0m WARNING 08-28 10:25:58 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=3592119)[0;0m WARNING 08-28 10:25:58 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=3592125)[0;0m WARNING 08-28 10:25:58 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=3592129)[0;0m WARNING 08-28 10:25:58 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 08-28 10:25:58 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 08-28 10:25:58 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x736b372cba50>, local_subscribe_port=36573, local_sync_port=60901, remote_subscribe_port=None, remote_sync_port=None)
[1;36m(VllmWorkerProcess pid=3592117)[0;0m INFO 08-28 10:25:58 model_runner.py:680] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...
INFO 08-28 10:25:58 model_runner.py:680] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...
[1;36m(VllmWorkerProcess pid=3592129)[0;0m INFO 08-28 10:25:58 model_runner.py:680] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...
[1;36m(VllmWorkerProcess pid=3592125)[0;0m INFO 08-28 10:25:58 model_runner.py:680] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...
[1;36m(VllmWorkerProcess pid=3592123)[0;0m INFO 08-28 10:25:58 model_runner.py:680] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...
[1;36m(VllmWorkerProcess pid=3592121)[0;0m INFO 08-28 10:25:58 model_runner.py:680] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...
[1;36m(VllmWorkerProcess pid=3592119)[0;0m INFO 08-28 10:25:58 model_runner.py:680] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...
[1;36m(VllmWorkerProcess pid=3592127)[0;0m INFO 08-28 10:25:58 model_runner.py:680] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method load_model: CUDA out of memory. Tried to allocate 56.00 MiB. GPU  has a total capacity of 22.09 GiB of which 7.38 MiB is free. Process 3592124 has 16.86 GiB memory in use. Including non-PyTorch memory, this process has 5.20 GiB memory in use. Of the allocated memory 4.80 GiB is allocated by PyTorch, and 22.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables), Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py", line 223, in _run_worker_process
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/worker.py", line 139, in load_model
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 682, in load_model
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model = get_model(model_config=self.model_config,
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 21, in get_model
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return loader.load_model(model_config=model_config,
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 280, in load_model
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     model = _initialize_model(model_config, self.load_config,
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 111, in _initialize_model
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return model_class(config=model_config.hf_config,
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 384, in __init__
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model = LlamaModel(config,
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                  ^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 285, in __init__
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                                                     ^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 144, in make_layers
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     [PPMissingLayer() for _ in range(start_layer)] + [
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                                                      ^
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 145, in <listcomp>
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 287, in <lambda>
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     lambda prefix: LlamaDecoderLayer(config=config,
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 217, in __init__
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.mlp = LlamaMLP(
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                ^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 76, in __init__
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.down_proj = RowParallelLinear(input_size=intermediate_size,
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 728, in __init__
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 109, in create_weights
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/torch/utils/_device.py", line 78, in __torch_function__
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU  has a total capacity of 22.09 GiB of which 7.38 MiB is free. Process 3592124 has 16.86 GiB memory in use. Including non-PyTorch memory, this process has 5.20 GiB memory in use. Of the allocated memory 4.80 GiB is allocated by PyTorch, and 22.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=3592125)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] 
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method load_model: CUDA out of memory. Tried to allocate 56.00 MiB. GPU  has a total capacity of 22.09 GiB of which 55.38 MiB is free. Process 3592132 has 16.84 GiB memory in use. Including non-PyTorch memory, this process has 5.18 GiB memory in use. Of the allocated memory 4.80 GiB is allocated by PyTorch, and 22.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables), Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py", line 223, in _run_worker_process
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/worker.py", line 139, in load_model
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 682, in load_model
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model = get_model(model_config=self.model_config,
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 21, in get_model
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return loader.load_model(model_config=model_config,
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 280, in load_model
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     model = _initialize_model(model_config, self.load_config,
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 111, in _initialize_model
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return model_class(config=model_config.hf_config,
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 384, in __init__
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model = LlamaModel(config,
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                  ^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 285, in __init__
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                                                     ^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 144, in make_layers
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     [PPMissingLayer() for _ in range(start_layer)] + [
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                                                      ^
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 145, in <listcomp>
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 287, in <lambda>
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     lambda prefix: LlamaDecoderLayer(config=config,
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 217, in __init__
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.mlp = LlamaMLP(
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                ^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 76, in __init__
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.down_proj = RowParallelLinear(input_size=intermediate_size,
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 728, in __init__
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 109, in create_weights
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/torch/utils/_device.py", line 78, in __torch_function__
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU  has a total capacity of 22.09 GiB of which 55.38 MiB is free. Process 3592132 has 16.84 GiB memory in use. Including non-PyTorch memory, this process has 5.18 GiB memory in use. Of the allocated memory 4.80 GiB is allocated by PyTorch, and 22.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=3592129)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] 
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method load_model: CUDA out of memory. Tried to allocate 56.00 MiB. GPU  has a total capacity of 22.09 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 5.20 GiB memory in use. Process 3592118 has 16.86 GiB memory in use. Of the allocated memory 4.80 GiB is allocated by PyTorch, and 22.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables), Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py", line 223, in _run_worker_process
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/worker.py", line 139, in load_model
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 682, in load_model
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model = get_model(model_config=self.model_config,
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 21, in get_model
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return loader.load_model(model_config=model_config,
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 280, in load_model
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     model = _initialize_model(model_config, self.load_config,
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 111, in _initialize_model
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return model_class(config=model_config.hf_config,
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 384, in __init__
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model = LlamaModel(config,
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                  ^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 285, in __init__
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                                                     ^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 144, in make_layers
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     [PPMissingLayer() for _ in range(start_layer)] + [
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                                                      ^
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 145, in <listcomp>
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 287, in <lambda>
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     lambda prefix: LlamaDecoderLayer(config=config,
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 217, in __init__
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.mlp = LlamaMLP(
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                ^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 76, in __init__
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.down_proj = RowParallelLinear(input_size=intermediate_size,
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 728, in __init__
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 109, in create_weights
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/torch/utils/_device.py", line 78, in __torch_function__
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU  has a total capacity of 22.09 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 5.20 GiB memory in use. Process 3592118 has 16.86 GiB memory in use. Of the allocated memory 4.80 GiB is allocated by PyTorch, and 22.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=3592119)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] 
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method load_model: CUDA out of memory. Tried to allocate 56.00 MiB. GPU  has a total capacity of 22.09 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 5.20 GiB memory in use. Process 3592116 has 16.86 GiB memory in use. Of the allocated memory 4.80 GiB is allocated by PyTorch, and 22.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables), Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py", line 223, in _run_worker_process
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/worker.py", line 139, in load_model
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 682, in load_model
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model = get_model(model_config=self.model_config,
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 21, in get_model
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return loader.load_model(model_config=model_config,
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 280, in load_model
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     model = _initialize_model(model_config, self.load_config,
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 111, in _initialize_model
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return model_class(config=model_config.hf_config,
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 384, in __init__
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model = LlamaModel(config,
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                  ^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 285, in __init__
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                                                     ^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 144, in make_layers
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     [PPMissingLayer() for _ in range(start_layer)] + [
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                                                      ^
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 145, in <listcomp>
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 287, in <lambda>
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     lambda prefix: LlamaDecoderLayer(config=config,
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 217, in __init__
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.mlp = LlamaMLP(
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                ^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 76, in __init__
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.down_proj = RowParallelLinear(input_size=intermediate_size,
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 728, in __init__
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 109, in create_weights
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/torch/utils/_device.py", line 78, in __torch_function__
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU  has a total capacity of 22.09 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 5.20 GiB memory in use. Process 3592116 has 16.86 GiB memory in use. Of the allocated memory 4.80 GiB is allocated by PyTorch, and 22.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=3592117)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] 
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method load_model: CUDA out of memory. Tried to allocate 56.00 MiB. GPU  has a total capacity of 22.09 GiB of which 55.38 MiB is free. Process 3592120 has 16.84 GiB memory in use. Including non-PyTorch memory, this process has 5.18 GiB memory in use. Of the allocated memory 4.80 GiB is allocated by PyTorch, and 22.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables), Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py", line 223, in _run_worker_process
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/worker.py", line 139, in load_model
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 682, in load_model
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model = get_model(model_config=self.model_config,
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 21, in get_model
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return loader.load_model(model_config=model_config,
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 280, in load_model
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     model = _initialize_model(model_config, self.load_config,
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 111, in _initialize_model
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return model_class(config=model_config.hf_config,
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 384, in __init__
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model = LlamaModel(config,
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                  ^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 285, in __init__
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                                                     ^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 144, in make_layers
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     [PPMissingLayer() for _ in range(start_layer)] + [
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                                                      ^
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 145, in <listcomp>
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 287, in <lambda>
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     lambda prefix: LlamaDecoderLayer(config=config,
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 217, in __init__
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.mlp = LlamaMLP(
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                ^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 76, in __init__
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.down_proj = RowParallelLinear(input_size=intermediate_size,
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 728, in __init__
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 109, in create_weights
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/torch/utils/_device.py", line 78, in __torch_function__
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU  has a total capacity of 22.09 GiB of which 55.38 MiB is free. Process 3592120 has 16.84 GiB memory in use. Including non-PyTorch memory, this process has 5.18 GiB memory in use. Of the allocated memory 4.80 GiB is allocated by PyTorch, and 22.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=3592121)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] 
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method load_model: CUDA out of memory. Tried to allocate 56.00 MiB. GPU  has a total capacity of 22.09 GiB of which 55.38 MiB is free. Process 3592122 has 16.84 GiB memory in use. Including non-PyTorch memory, this process has 5.18 GiB memory in use. Of the allocated memory 4.80 GiB is allocated by PyTorch, and 22.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables), Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py", line 223, in _run_worker_process
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/worker.py", line 139, in load_model
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 682, in load_model
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model = get_model(model_config=self.model_config,
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 21, in get_model
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return loader.load_model(model_config=model_config,
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 280, in load_model
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     model = _initialize_model(model_config, self.load_config,
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 111, in _initialize_model
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return model_class(config=model_config.hf_config,
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 384, in __init__
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model = LlamaModel(config,
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                  ^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 285, in __init__
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                                                     ^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 144, in make_layers
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     [PPMissingLayer() for _ in range(start_layer)] + [
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                                                      ^
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 145, in <listcomp>
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 287, in <lambda>
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     lambda prefix: LlamaDecoderLayer(config=config,
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 217, in __init__
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.mlp = LlamaMLP(
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                ^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 76, in __init__
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.down_proj = RowParallelLinear(input_size=intermediate_size,
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 728, in __init__
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 109, in create_weights
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/torch/utils/_device.py", line 78, in __torch_function__
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU  has a total capacity of 22.09 GiB of which 55.38 MiB is free. Process 3592122 has 16.84 GiB memory in use. Including non-PyTorch memory, this process has 5.18 GiB memory in use. Of the allocated memory 4.80 GiB is allocated by PyTorch, and 22.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=3592123)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] 
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method load_model: CUDA out of memory. Tried to allocate 56.00 MiB. GPU  has a total capacity of 22.09 GiB of which 7.38 MiB is free. Process 3592126 has 16.86 GiB memory in use. Including non-PyTorch memory, this process has 5.20 GiB memory in use. Of the allocated memory 4.80 GiB is allocated by PyTorch, and 22.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables), Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py", line 223, in _run_worker_process
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/worker.py", line 139, in load_model
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 682, in load_model
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model = get_model(model_config=self.model_config,
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 21, in get_model
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return loader.load_model(model_config=model_config,
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 280, in load_model
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     model = _initialize_model(model_config, self.load_config,
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 111, in _initialize_model
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return model_class(config=model_config.hf_config,
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 384, in __init__
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.model = LlamaModel(config,
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                  ^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 285, in __init__
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                                                     ^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 144, in make_layers
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     [PPMissingLayer() for _ in range(start_layer)] + [
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                                                      ^
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 145, in <listcomp>
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 287, in <lambda>
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     lambda prefix: LlamaDecoderLayer(config=config,
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 217, in __init__
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.mlp = LlamaMLP(
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                ^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 76, in __init__
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.down_proj = RowParallelLinear(input_size=intermediate_size,
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 728, in __init__
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 109, in create_weights
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/torch/utils/_device.py", line 78, in __torch_function__
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU  has a total capacity of 22.09 GiB of which 7.38 MiB is free. Process 3592126 has 16.86 GiB memory in use. Including non-PyTorch memory, this process has 5.20 GiB memory in use. Of the allocated memory 4.80 GiB is allocated by PyTorch, and 22.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=3592127)[0;0m ERROR 08-28 10:25:58 multiproc_worker_utils.py:226] 
[rank0]: Traceback (most recent call last):
[rank0]:   File "/common/users/hz624/AgentAttackBench/main_attacker.py", line 318, in <module>
[rank0]:     main()
[rank0]:   File "/common/users/hz624/AgentAttackBench/main_attacker.py", line 136, in main
[rank0]:     llm = llms.LLMKernel(
[rank0]:           ^^^^^^^^^^^^^^^
[rank0]:   File "/common/users/hz624/AgentAttackBench/aios/llm_core/llms.py", line 40, in __init__
[rank0]:     self.model = vLLM(
[rank0]:                  ^^^^^
[rank0]:   File "/common/users/hz624/AgentAttackBench/aios/llm_core/llm_classes/vllm.py", line 23, in __init__
[rank0]:     super().__init__(llm_name,
[rank0]:   File "/common/users/hz624/AgentAttackBench/aios/llm_core/llm_classes/base_llm.py", line 30, in __init__
[rank0]:     self.load_llm_and_tokenizer()
[rank0]:   File "/common/users/hz624/AgentAttackBench/aios/llm_core/llm_classes/vllm.py", line 42, in load_llm_and_tokenizer
[rank0]:     self.model = vllm.LLM(
[rank0]:                  ^^^^^^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 155, in __init__
[rank0]:     self.llm_engine = LLMEngine.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 441, in from_engine_args
[rank0]:     engine = cls(
[rank0]:              ^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 251, in __init__
[rank0]:     self.model_executor = executor_class(
[rank0]:                           ^^^^^^^^^^^^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py", line 25, in __init__
[rank0]:     super().__init__(*args, **kwargs)
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/executor/executor_base.py", line 47, in __init__
[rank0]:     self._init_executor()
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py", line 124, in _init_executor
[rank0]:     self._run_workers("load_model",
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py", line 178, in _run_workers
[rank0]:     driver_worker_output = driver_worker_method(*args, **kwargs)
[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/worker.py", line 139, in load_model
[rank0]:     self.model_runner.load_model()
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 682, in load_model
[rank0]:     self.model = get_model(model_config=self.model_config,
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 21, in get_model
[rank0]:     return loader.load_model(model_config=model_config,
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 280, in load_model
[rank0]:     model = _initialize_model(model_config, self.load_config,
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 111, in _initialize_model
[rank0]:     return model_class(config=model_config.hf_config,
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 384, in __init__
[rank0]:     self.model = LlamaModel(config,
[rank0]:                  ^^^^^^^^^^^^^^^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 285, in __init__
[rank0]:     self.start_layer, self.end_layer, self.layers = make_layers(
[rank0]:                                                     ^^^^^^^^^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 144, in make_layers
[rank0]:     [PPMissingLayer() for _ in range(start_layer)] + [
[rank0]:                                                      ^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 145, in <listcomp>
[rank0]:     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[rank0]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 287, in <lambda>
[rank0]:     lambda prefix: LlamaDecoderLayer(config=config,
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 217, in __init__
[rank0]:     self.mlp = LlamaMLP(
[rank0]:                ^^^^^^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 76, in __init__
[rank0]:     self.down_proj = RowParallelLinear(input_size=intermediate_size,
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 728, in __init__
[rank0]:     self.quant_method.create_weights(
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 109, in create_weights
[rank0]:     weight = Parameter(torch.empty(sum(output_partition_sizes),
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/site-packages/torch/utils/_device.py", line 78, in __torch_function__
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 
ERROR 08-28 10:25:59 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 3592127 died, exit code: -15
INFO 08-28 10:25:59 multiproc_worker_utils.py:123] Killing local vLLM worker processes
/common/home/hz624/anaconda3/envs/AIOS/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
