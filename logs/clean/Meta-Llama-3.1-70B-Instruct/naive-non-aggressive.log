The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /filer/tmp1/hz624/.cache/huggingface/hub/token
Login successful
Attack started at: 2024-08-28 12:39
Namespace(llm_name='meta-llama/Meta-Llama-3.1-70B-Instruct', max_gpu_memory={'0': '24GB', '1': '24GB', '2': '24GB', '3': '24GB', '4': '24GB', '5': '24GB', '6': '24GB', '7': '24GB'}, eval_device='["cuda:7", "cuda:6", "cuda:5", "cuda:4", "cuda:3", "cuda:2", "cuda:1", "cuda:0"]', max_new_tokens=1024, scheduler_log_mode='console', agent_log_mode='console', llm_kernel_log_mode='console', use_backend='vllm', workflow_mode='automatic', attacker_tools_path='data/all_attack_tools_non_aggressive.jsonl', tasks_path='data/agent_task.jsonl', tools_info_path='data/all_normal_tools.jsonl', observation_prompt_injection=False, plan_attack=False, action_attack=False, direct_prompt_injection=False, jailbreak=False, attack_type='naive', cot_backdoor=False, cot_clean=False, target='ResearcherProfileManager', trigger='in light of extant literature', memory_attack=False, clean=True, result_file='logs/clean/Meta-Llama-3.1-70B-Instruct/result_statistics.log', database='memory_db/chroma_db', task_num=1)
INFO 08-28 12:39:16 config.py:715] Defaulting to use mp for distributed inference
WARNING 08-28 12:39:16 arg_utils.py:762] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 08-28 12:39:16 config.py:806] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 08-28 12:39:16 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='meta-llama/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=50000, download_dir='/filer/tmp1/hz624/.cache/huggingface/hub/', load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 08-28 12:39:16 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=3699029)[0;0m INFO 08-28 12:39:17 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=3699027)[0;0m INFO 08-28 12:39:17 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=3699026)[0;0m INFO 08-28 12:39:17 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=3699028)[0;0m INFO 08-28 12:39:17 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=3699030)[0;0m INFO 08-28 12:39:17 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=3699031)[0;0m INFO 08-28 12:39:17 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=3699032)[0;0m INFO 08-28 12:39:17 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
INFO 08-28 12:39:19 utils.py:784] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3699027)[0;0m INFO 08-28 12:39:19 utils.py:784] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3699026)[0;0m INFO 08-28 12:39:19 utils.py:784] Found nccl from library libnccl.so.2
INFO 08-28 12:39:19 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=3699028)[0;0m INFO 08-28 12:39:19 utils.py:784] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3699029)[0;0m INFO 08-28 12:39:19 utils.py:784] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3699030)[0;0m INFO 08-28 12:39:19 utils.py:784] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3699031)[0;0m INFO 08-28 12:39:19 utils.py:784] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3699027)[0;0m INFO 08-28 12:39:19 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=3699026)[0;0m INFO 08-28 12:39:19 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=3699032)[0;0m INFO 08-28 12:39:19 utils.py:784] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=3699029)[0;0m INFO 08-28 12:39:19 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=3699028)[0;0m INFO 08-28 12:39:19 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=3699030)[0;0m INFO 08-28 12:39:19 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=3699031)[0;0m INFO 08-28 12:39:19 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=3699032)[0;0m INFO 08-28 12:39:19 pynccl.py:63] vLLM is using nccl==2.20.5
